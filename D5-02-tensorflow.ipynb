{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D5 - 02 -Tensorflow\n",
    "\n",
    "## Content\n",
    "- Installation and introduction\n",
    "- Session\n",
    "- Computational graphs\n",
    "- Operations and tensors\n",
    "- Placeholders and Variables\n",
    "\n",
    "\n",
    "\n",
    "## Remember jupyter notebooks\n",
    "- To run the currently highlighted cell, hold <kbd>&#x21E7; Shift</kbd> and press <kbd>&#x23ce; Enter</kbd>.\n",
    "- To get help for a specific function, place the cursor within the function's brackets, hold <kbd>&#x21E7; Shift</kbd>, and press <kbd>&#x21E5; Tab</kbd>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation instructions\n",
    "\n",
    "On Linux/OSX:\n",
    "\n",
    "- Open your command prompt\n",
    "- \"pip install tensorflow\"\n",
    "\n",
    "On Windows:\n",
    "\n",
    "- Open your anaconda prompt\n",
    "- \"pip install --upgrade tensorflow\"\n",
    "\n",
    "Then run the cell below to make sure that tensorflow was correctly installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# You should have version 1.7.0\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Core Concept: Sessions\n",
    "\n",
    "In TensorFlow, the execution of operations and evaluation of tensors may only be performed in a special environment referred to as **session**. Sessions are responsible for allocating and managing resources, variables, and more aspects of the execution of our code.\n",
    "\n",
    "A tensorflow session has to be created as an instance of the tf.Session() class (it can be configured, but this is too advanced for now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_session = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After initializing our session, it will be available as an environment in which to execute our operations using the session.run() method. This methods accepts as parameters a list of **fetches** and a dictionary of **feeds**, being respectively the outputs that we want calculated and the inputs that are required for it. session.run() will then return the result obtained on the **fetches** nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will clearly fail, as we didnt define our inputs and outputs\n",
    "result = my_session.run(outputs_we_want, inputs_we_have)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just need to learn how to define algorithms in tf!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational graphs\n",
    "\n",
    "In TensorFlow, algorithms are represented as **computational graphs**. \n",
    "\n",
    "In such graphs **nodes** represent operations, while arrows represent data flowing between these operation; these data are also called **tensors**.\n",
    "\n",
    "This representation is extremely intuitive to visualize; moreover, an operation in tensorflow can be pretty much anything, thus the great flexibility of the library.\n",
    "\n",
    "<img src=\"graphs.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "\n",
    "Tensors are multidimentional collections of values that share a common type. In simpler words, they are 'containers' for a certain number of values, arranged in a determined shape. Examples:\n",
    "\n",
    "- scalars -->  tensors of 1 value with shape [0]\n",
    "- vetors -->  tensors of $N$ values with shape [N]\n",
    "- matrices --> tensors of $NxM$ values with shape [N, M]\n",
    "\n",
    "and so on.\n",
    "\n",
    "Until we execute our graph, tensors are not holding any values, but they are just containers; when we run the structure made of tensors and operations, aka our graph, inside the session, tensors are used to process values in a machine-efficient way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "\n",
    "We can now try and implement our very first simple graph. We'll start easy and just get the result of 2 + 3.\n",
    "\n",
    "What we need to do here is to create a tensor that will hold the value '2', another for the value '3', and a third tensor for their '+' operation. We will now run and fetch from the session the output of the 'sum' node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use tf.constant to create our tensors starting from a given value. Note that this value can also be\n",
    "# an iterable such a list or numpy array\n",
    "a = tf.constant(2)\n",
    "b = tf.constant(3)\n",
    "\n",
    "c = tf.add(a, b)\n",
    "\n",
    "# Note: this also works\n",
    "d = a + b\n",
    "\n",
    "# Note that if we print c we don't get 5, we just get the information that it's actually a tensor!\n",
    "print(c)\n",
    "\n",
    "# Now we run c and d inside the session and print the output value\n",
    "print(my_session.run([c,d]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Let's try something more complicated: use the tf.multiply() and tf.reduce_sum() operations to implement the scalar product of [1,0,1,0,1,-1] and [0,1,0,1,1,1], and verify that the output is zero.\n",
    "\n",
    "Note: tf.reduce_sum() has the same behaviour as np.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Rebuild the pi estimator, but this time use tensorflow functions to generate the points and their norm. Use numpy to count the elements inside the circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, tensorflow is not very useful except for being a very complicated way of calculating 2 + 3. We need to introduce the concept of **placeholder** to make it more relevant.\n",
    "\n",
    "Placeholders are what we use to pass values to our network. This makes tensorflow much more useful as we can now build complex computational graphs that rely on an undetermined input, then pass the input value only when we actually run the code. The only thing we need to set when we create the placeholder is the data type it will have to contain.\n",
    "\n",
    "If we want our code to be more optimized we can also set a shape for the placeholder (this way the session will know how much memory to allocate for it); this is not necessary, but it's a good practice for when we want our input data to have a fixed size.\n",
    "\n",
    "If we want some of the dimensions to be fixed and others to change, we can use a value for the fixed one, and place a 'None' in the position of the variable dimension.\n",
    "\n",
    "This is done by creating a dictionary that contains tensor names and their corresponding inputs, and passing it as the second argument of session.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For defining the type we can use the tf.float32 type, which behaves in the same way as a numpy float32.\n",
    "# We want the inputs to be scalars. This is a particular case, and we have to set the shape to ()\n",
    "a = tf.placeholder(tf.float32, shape = ())\n",
    "b = tf.placeholder(tf.float32, shape = ())\n",
    "\n",
    "c = a + b\n",
    "\n",
    "inputs_dict = {\n",
    "    a : 2,\n",
    "    b : 3\n",
    "}\n",
    "\n",
    "print(my_session.run(c, inputs_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Implement the scalar product that we constructed before, but have it accept two arrays instead of using two constant ones. Set the shape of the placeholders so that they only accept 1D arrays of variable size.\n",
    "\n",
    "Note: scalars will pass through if\n",
    "\n",
    "shape = (None)\n",
    "\n",
    "but will raise an error if\n",
    "\n",
    "shape = (None,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Rebuild the linear regression example using only tensorflow:\n",
    "\n",
    "$$\\begin{eqnarray*}\n",
    "\\textrm{slope} & = & \\frac{\\sum_{n=0}^{N-1} \\left( x_n - \\bar{x} \\middle) \\middle( y_n - \\bar{y} \\right)}{\\sum_{n=0}^{N-1} \\left( x_n - \\bar{x} \\right)^2} \\\\\n",
    "\\textrm{const} & = & \\bar{y} - \\textrm{slope } \\bar{x}\n",
    "\\end{eqnarray*}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now introduce the third concept that we need to build a machine learning system. When we train a ML algorithm, we have some internal variables (weights) that are updated to reach a certain solution. We cannot use tf.constant() for those as its value can't be changed. We could use placeholders and handle the process of update ourselves, but that would be a huge hassle.\n",
    "\n",
    "Tensorflow solves this with the **Variable** class. These are persistent but mutable handles, whose value survives along with our session, but can be easily modified. They are characterized by an unmutable data type and shape; we don't need to set them, as they are automatically inferred by the variable's initializer.\n",
    "\n",
    "Variable() has lots of options to regulate its creation, but the most important one is its **initialization**. This is the value that the variable will be assigned when we decide to give the variable a new value; we have to do this before using it, but it can also be used to reset a variable to its initial value. \n",
    "\n",
    "Initialization is done through running inside a session a specific kind of node, which can be used to regulate which variables we want to set/reset. For simple cases, we can run the node tf.global_variables_initializer(), which simply reset every variable in our graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "initial_w = np.arange(9).reshape(3, 3)\n",
    "\n",
    "# First we create the variable\n",
    "w = tf.Variable(initial_w)\n",
    "\n",
    "# Now we create the initializer tensor\n",
    "initializer = tf.global_variables_initializer()\n",
    "\n",
    "# Note: running the following line will give us an error,\n",
    "# as we are using a variable before having set a value for it!\n",
    "print(my_session.run(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we run first the initializer\n",
    "my_session.run(initializer)\n",
    "\n",
    "# then the variable, everything works correctly\n",
    "print(my_session.run(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every variable has a method variable.assign(), which we can use to change its value. In practice though most of the updates to the variables will be automatic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor that updates the variable\n",
    "update_w = w.assign(np.arange(10,19).reshape(3,3))\n",
    "\n",
    "# Run our 'updater' tensor\n",
    "my_session.run(update_w)\n",
    "\n",
    "print(my_session.run(w))\n",
    "\n",
    "# after we rerun the initializer, w goes back to its default value\n",
    "my_session.run(initializer)\n",
    "\n",
    "print(my_session.run(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Build a graph that multiplies a scalar by a 1D array. Use a placeholder for the scalar and a variable for the array. Initialize the array to [1,0,1,0,1].\n",
    "\n",
    "Feed a value of 5 to the placeholder and print the result. Then update the variable to [0,1,0,1,0], and print the result while feeding a value of 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: sometimes it's good to reset our session, in order to free it from variables that are not needed anymore. You can close a session by calling its .close() method, but all its internal variables, tensors, and placeholders will be deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, however, how do we update automatically the weights of our network in tensorflow?\n",
    "\n",
    "First, let's define the problem we want it to solve. We generate points belonging to two different gaussians on the x,y plane, and we want our network to distinguish them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = np.random.multivariate_normal([4,3], cov = np.eye(2,2), size = 5000)\n",
    "g2 = np.random.multivariate_normal([10,3], cov = np.eye(2,2), size = 5000)\n",
    "\n",
    "plt.scatter(g1[:,0], g1[:,1], s = 1)\n",
    "plt.scatter(g2[:,0], g2[:,1], s = 1)\n",
    "plt.show()\n",
    "\n",
    "all_datapoints = np.concatenate((g1,g2), axis = 0)\n",
    "labels = np.concatenate((np.zeros(5000), np.ones(5000))).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need our network to assign a single value to each (x,y) pair, so we want a (2,1)-sized weights matrix and a (1,)-sized bias array.\n",
    "\n",
    "We want to initialize them to small, nonzero values, so we can use as initializer tf.random_normal()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable(tf.random_normal(shape = [2,1]))\n",
    "b = tf.Variable(tf.random_normal(shape = [1,]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to define a placeholder for our input. We want it to accept a variable number of 2-D points, so we set the shape to (None, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape = (None, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want our network to only output values between 0 and 1, so we use a sigmoid function to constrain the output between these two values. This function, along with many other useful ones, can be found in the tf.nn package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_network = tf.nn.sigmoid(x @ w + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need another placeholder for the labels, as we will need to feed those to the network too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_label = tf.placeholder(tf.float32, shape = (None,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define the loss function, which our network is going to optimize the weights for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.mean_squared_error(y_label, y_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally introduce the tool that will update the weights of the network, the tensorflow optimizer, which requires a learning rate to be initialized. The optimizer's 'minimize' method, which needs a tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: when setting the learning rate, a rule of thumb is 'the bigger the network, the smaller the learning rate'\n",
    "learning_rate = 1e-1\n",
    "training_tensor = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now initialize our variables (Note: we have to do this after we create all the variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_init = tf.global_variables_initializer()\n",
    "sess.run(global_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can train our network. We can do this by running the training_tensor we created earlier inside a tensorflow session, along with the nodes we want to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "for epoch_index in range(epochs):\n",
    "    feed_dict = {\n",
    "        x : all_datapoints,\n",
    "        y_label : labels\n",
    "    }\n",
    "    \n",
    "    sess.run(training_tensor, feed_dict)\n",
    "    \n",
    "    # We also want to visualize every 20 epochs how the network is classifying the two gaussians\n",
    "    if epoch_index % 20 == 1:\n",
    "        y_rec = sess.run(y_network, {x: all_datapoints})\n",
    "\n",
    "        g1_rec = all_datapoints[np.where(y_rec[:,0] > 0.5)]\n",
    "        g2_rec = all_datapoints[np.where(y_rec[:,0] < 0.5)]\n",
    "        \n",
    "        print(g1_rec.shape)\n",
    "        print(g2_rec.shape)\n",
    "                \n",
    "        if g1_rec.size > 0:\n",
    "            plt.scatter(g1_rec[:,0], g1_rec[:,1], s = 1)\n",
    "            \n",
    "        if g2_rec.size > 0:\n",
    "            plt.scatter(g2_rec[:,0], g2_rec[:,1], s = 1)\n",
    "            \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Generate points from a 2-dimensional gaussian distribution centered around (0,0) with covariance matrix = [[1,0],[0,1]. Generate labels for each point so that:\n",
    "\n",
    "- label = 1, x*y > 0\n",
    "- 0, x*y < 0\n",
    "\n",
    "Note that this is not a linear problem, so it cannot be solved by a network with a single linear layer. \n",
    "\n",
    "Build a 3-layer network in tensorflow, and train it so that it will correctly classify each point. A network with layers with size 8 should be sufficient. Play around with the learning rate and number of epochs until you manage to have the network converge to a result you consider acceptable.\n",
    "\n",
    "Tip: One of the best functions to be used as activations for intermediate layers is the 'ReLU' function, that you can find in the tf.nn package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Now build a new network that will manage to fit the nonlinear function given below. You can do this by using setting the input size to 1 and output size to 1, and use a linear activation as output so that your network can generate basically any value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(length, noise=0.1):\n",
    "    x = 2.0 * (np.random.rand(length) - 0.5) * np.pi * 1.5\n",
    "    f = lambda x: np.tanh(x * 0.5) * np.exp(-x**2)\n",
    "    y = f(x) + np.random.randn(len(x)) * noise\n",
    "    return x, y, f\n",
    "\n",
    "x, y, f = gen_data(2000)\n",
    "x2 = np.linspace(x.min(), x.max(), 100)\n",
    "y2 = f(x2)\n",
    "\n",
    "plt.scatter(x, y, s=0.5)\n",
    "plt.plot(x2, y2, linewidth=2, color='C1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
