{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D5 - 01 - Modeling with Deep Learning\n",
    "\n",
    "## Content\n",
    "- Designing neural networks for **fitting** noisy data\n",
    "- Hyperparameter optimisation\n",
    "- Splitting datasets\n",
    "- Regularisation with dropout\n",
    "\n",
    "## Remember jupyter notebooks\n",
    "- To run the currently highlighted cell, hold <kbd>&#x21E7; Shift</kbd> and press <kbd>&#x23ce; Enter</kbd>.\n",
    "- To get help for a specific function, place the cursor within the function's brackets, hold <kbd>&#x21E7; Shift</kbd>, and press <kbd>&#x21E5; Tab</kbd>.\n",
    "\n",
    "## A notebook \"preamble\"\n",
    "The first code block prepares our notebook by specifying how to render plots and importing the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a noisy dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(length, noise=0.1):\n",
    "    x = 2.0 * (np.random.rand(length) - 0.5) * np.pi * 1.5\n",
    "    f = lambda x: np.tanh(x * 0.5) * np.exp(-x**2)\n",
    "    y = f(x) + np.random.randn(len(x)) * noise\n",
    "    return x, y, f\n",
    "\n",
    "x, y, f = gen_data(5000)\n",
    "x2 = np.linspace(x.min(), x.max(), 100)\n",
    "y2 = f(x2)\n",
    "\n",
    "plt.scatter(x, y, s=0.5, alpha=0.3)\n",
    "plt.plot(x2, y2, linewidth=2, color='C1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... which we cast into `torch.autograd.Variable`s for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_trn = torch.autograd.Variable(\n",
    "    torch.Tensor(x.reshape(-1, 1)))\n",
    "out_trn = torch.autograd.Variable(\n",
    "    torch.Tensor(y.reshape(-1, 1)))\n",
    "\n",
    "print(inp_trn)\n",
    "print(out_trn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define a first version of out neural network for fitting functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layer1 = torch.nn.Linear(1, 10)\n",
    "        self.layer2 = torch.nn.Linear(10, 30)\n",
    "        self.layer3 = torch.nn.Linear(30, 50)\n",
    "        self.layer4 = torch.nn.Linear(50, 30)\n",
    "        self.layer5 = torch.nn.Linear(30, 10)\n",
    "        self.layer6 = torch.nn.Linear(10, 1)\n",
    "        self.activation = torch.nn.LeakyReLU()\n",
    "    def forward(self, inp):\n",
    "        x = self.activation(self.layer1(inp))\n",
    "        x = self.activation(self.layer2(x))\n",
    "        x = self.activation(self.layer3(x))\n",
    "        x = self.activation(self.layer4(x))\n",
    "        x = self.activation(self.layer5(x))\n",
    "        return self.layer6(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instanciate the network and train it on the full set of data; then, we visualise the `loss` history and show the network's prediction for the full dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_fit = NeuralNetwork()\n",
    "loss_function = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(deep_fit.parameters(), lr=0.0001)\n",
    "\n",
    "loss_trn = []\n",
    "deep_fit.train()\n",
    "for _ in range(5000):\n",
    "    loss = loss_function(deep_fit(inp_trn), out_trn)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    loss_trn.append(loss.data[0])\n",
    "deep_fit.eval()\n",
    "\n",
    "fig, (cnv, prd) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "cnv.plot(loss_trn)\n",
    "cnv.set_xlabel('epoch')\n",
    "cnv.set_ylabel('loss')\n",
    "prd.scatter(\n",
    "    inp_trn.data.numpy(),\n",
    "    deep_fit(inp_trn).data.numpy(),\n",
    "    s=1, c='C4')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating data for training/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn = np.random.choice(x.size, int(0.7 * x.size), replace=False)\n",
    "val = np.setdiff1d(np.arange(x.size), trn, assume_unique=True)\n",
    "\n",
    "plt.scatter(x[trn], y[trn], s=0.5)\n",
    "plt.scatter(x[val], y[val], s=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to define new `torch.autograd.Variable`s for training...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_trn = torch.autograd.Variable(\n",
    "    torch.Tensor(x[trn].reshape(-1, 1)))\n",
    "out_trn = torch.autograd.Variable(\n",
    "    torch.Tensor(y[trn].reshape(-1, 1)))\n",
    "\n",
    "print(inp_trn)\n",
    "print(out_trn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and validation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_val = torch.autograd.Variable(\n",
    "    torch.Tensor(x[val].reshape(-1, 1)),\n",
    "    volatile=True)\n",
    "out_val = torch.autograd.Variable(\n",
    "    torch.Tensor(y[val].reshape(-1, 1)),\n",
    "    volatile=True)\n",
    "\n",
    "print(inp_val)\n",
    "print(out_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and we redo the training for a new instance of our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_fit = NeuralNetwork()\n",
    "loss_function = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(deep_fit.parameters(), lr=0.0001)\n",
    "\n",
    "loss_trn, loss_val = [], []\n",
    "for _ in range(5000):\n",
    "    deep_fit.train()\n",
    "    loss = loss_function(deep_fit(inp_trn), out_trn)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    loss_trn.append(loss.data[0])\n",
    "    deep_fit.eval()\n",
    "    loss = loss_function(deep_fit(inp_val), out_val)\n",
    "    loss_val.append(loss.data[0])\n",
    "\n",
    "fig, (cnv, prd) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "cnv.plot(loss_trn, label='training')\n",
    "cnv.plot(loss_val, label='validation')\n",
    "cnv.set_xlabel('epoch')\n",
    "cnv.set_ylabel('loss')\n",
    "cnv.legend()\n",
    "prd.scatter(\n",
    "    inp_val.data.numpy(),\n",
    "    deep_fit(inp_val).data.numpy(),\n",
    "    s=1, c='C4')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layer1 = torch.nn.Linear(1, 10)\n",
    "        self.layer2 = torch.nn.Linear(10, 30)\n",
    "        self.layer3 = torch.nn.Linear(30, 50)\n",
    "        self.layer4 = torch.nn.Linear(50, 30)\n",
    "        self.layer5 = torch.nn.Linear(30, 10)\n",
    "        self.layer6 = torch.nn.Linear(10, 1)\n",
    "        self.activation = torch.nn.LeakyReLU()\n",
    "        self.dropout = torch.nn.Dropout(p=0.1)\n",
    "    def forward(self, inp):\n",
    "        x = self.dropout(self.activation(self.layer1(inp)))\n",
    "        x = self.dropout(self.activation(self.layer2(x)))\n",
    "        x = self.dropout(self.activation(self.layer3(x)))\n",
    "        x = self.dropout(self.activation(self.layer4(x)))\n",
    "        x = self.dropout(self.activation(self.layer5(x)))\n",
    "        return self.layer6(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_fit = NeuralNetwork()\n",
    "loss_function = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(deep_fit.parameters(), lr=0.0001)\n",
    "\n",
    "loss_trn, loss_val = [], []\n",
    "for _ in range(5000):\n",
    "    deep_fit.train()\n",
    "    loss = loss_function(deep_fit(inp_trn), out_trn)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    loss_trn.append(loss.data[0])\n",
    "    deep_fit.eval()\n",
    "    loss = loss_function(deep_fit(inp_val), out_val)\n",
    "    loss_val.append(loss.data[0])\n",
    "\n",
    "fig, (cnv, prd) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "cnv.plot(loss_trn, label='training')\n",
    "cnv.plot(loss_val, label='validation')\n",
    "cnv.set_xlabel('epoch')\n",
    "cnv.set_ylabel('loss')\n",
    "cnv.legend()\n",
    "prd.scatter(\n",
    "    inp_val.data.numpy(),\n",
    "    deep_fit(inp_val).data.numpy(),\n",
    "    s=1, c='C4')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turing off the training mode for making predictions (use `.eval()`) is very important:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_fit.train()\n",
    "plt.scatter(\n",
    "    inp_val.data.numpy(),\n",
    "    deep_fit(inp_val).data.numpy(),\n",
    "    s=1, c='C4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
